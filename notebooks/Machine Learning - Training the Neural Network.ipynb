{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Training the models using a LSTM based network\n",
    "\n",
    "First we are importing the necessary libraries and the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, model_selection\n",
    "from tensorflow import keras\n",
    "from tensorflow.metrics import precision\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "features = genfromtxt('../datasets/final-data/features.csv', delimiter=',')\n",
    "labels = genfromtxt('../datasets/final-data/labels.csv', delimiter=',')\n",
    "#embedding_matrix = genfromtxt('../datasets/final-data/embedding_matrix.csv', delimiter=',')\n",
    "\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(features,labels,test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2982, 900)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the models to be tested\n",
    "\n",
    "Now we are combining all the variable parameters and creating multiple models to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\gusta\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# input_dim = len(embedding_matrix)\n",
    "# input_length = len(train_x[0])\n",
    "\n",
    "# Variable parameters to be tested\n",
    "units_options = [150, 75, 35]\n",
    "activation_functions = [\"relu\", \"sigmoid\", \"tanh\"]\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "momentum_options = [0.1, 0.01, 0.001, 0.0001]\n",
    "decay_options = [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "#Create multiple models combining all the parameters\n",
    "models = []\n",
    "for units in units_options:\n",
    "    for learning_rate in learning_rates:\n",
    "        for momentum in momentum_options:\n",
    "            for activation_function in activation_functions:\n",
    "                for decay in decay_options:\n",
    "\n",
    "                    #Creating the network structure\n",
    "                    model = keras.Sequential()\n",
    "\n",
    "                    # Input, \n",
    "                    # Dense(linear(units = 150)), \n",
    "                    # Dense(relu), \n",
    "                    # batch_normalization(opcional, BatchNormalization1D), \n",
    "                    # linear(2)\n",
    "\n",
    "                    model.add(\n",
    "                        keras.layers.Input(\n",
    "                            shape=900,\n",
    "                            sparse=False\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "\n",
    "                    model.add(keras.layers.Dense(450))\n",
    "                    model.add(keras.layers.Dense(units, activation = activation_function))\n",
    "                    model.add(keras.layers.BatchNormalization())\n",
    "                    model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "                    # Setting the optimizers parameters\n",
    "                    optimizer = tf.keras.optimizers.SGD(\n",
    "                        learning_rate=learning_rate,\n",
    "                        decay=decay,\n",
    "                        momentum=momentum,\n",
    "                        nesterov=True\n",
    "                    )\n",
    "\n",
    "                    # Compiling the model\n",
    "                    model.compile(\n",
    "                        optimizer = optimizer,\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['acc', 'mae', 'mse'])\n",
    "\n",
    "                    # Including the new model in the models array\n",
    "                    models.append(\n",
    "                        {\n",
    "                            \"model_name\": 'model_' + str(len(models) + 1),\n",
    "                            \"units\": units, \n",
    "                            \"learning_rate\": learning_rate, \n",
    "                            \"momentum\": momentum,\n",
    "                            \"decay\": decay,\n",
    "                            \"activation_function\": 'relu',\n",
    "                            \"model\": model\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "\n",
    "pd.DataFrame(models) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the models\n",
    "\n",
    "Training all the created models and storing their performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "training_results = []\n",
    "\n",
    "trained_models = 1\n",
    "\n",
    "for model_data in models:\n",
    "    model = model_data[\"model\"]\n",
    "\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_x, \n",
    "        train_y, \n",
    "        epochs = 50, \n",
    "        validation_split=0.3, \n",
    "        batch_size = 16,  \n",
    "        verbose=1, \n",
    "        shuffle=True,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    loss, acc, mae, mse = model.evaluate(test_x, test_y, verbose=1)\n",
    "    \n",
    "    test_output_probabilities = model.predict(\n",
    "        test_x,\n",
    "        batch_size=16,\n",
    "        verbose=1,\n",
    "        steps=None,\n",
    "        callbacks=None,\n",
    "        max_queue_size=10,\n",
    "        workers=1,\n",
    "        use_multiprocessing=False\n",
    "    )\n",
    "    \n",
    "    predicted_y = np.argmax(test_output_probabilities, axis=1)\n",
    "    \n",
    "    precision, recall, fs_score, support = precision_recall_fscore_support(\n",
    "        y_true = test_y, \n",
    "        y_pred = predicted_y, \n",
    "        average = 'binary'\n",
    "    )\n",
    "\n",
    "    training_results.append(\n",
    "        {\n",
    "            \"model_name\": model_data[\"model_name\"],\n",
    "            \"units\": model_data[\"units\"], \n",
    "            \"learning_rate\": model_data[\"learning_rate\"], \n",
    "            \"momentum\": model_data[\"momentum\"],\n",
    "            \"decay\": model_data[\"decay\"],\n",
    "            \"activation_function\": model_data[\"activation_function\"],\n",
    "            \"model\": model,\n",
    "            \"history\": history,\n",
    "            \"acc\": acc,\n",
    "            \"loss\": loss,\n",
    "            \"mae\": mae,\n",
    "            \"mse\": mse,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"fs_score\": fs_score,\n",
    "            \"test_output_probabilities\": test_output_probabilities,\n",
    "            \"test_y\": test_y,\n",
    "            \"predicted_y\": predicted_y\n",
    "        }\n",
    "    )\n",
    "    print(\"Trained models: {}\".format(trained_models))\n",
    "    trained_models = trained_models + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a dataframe with the training metrics\n",
    "\n",
    "We are going to store those metrics in a separate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_metrics_df = pd.DataFrame(training_results)\n",
    "\n",
    "training_metrics_df = training_metrics_df[['model_name', 'units', 'learning_rate', 'momentum', 'decay', 'activation_function', 'acc', 'loss', 'mae', 'mse', 'precision', 'recall', 'fs_score']]\n",
    "\n",
    "training_metrics_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the general experiments files\n",
    "\n",
    "Saving the files with general data about the experiment:\n",
    "\n",
    "- All datasets used(train_x, train_y, test_x, test_y)\n",
    "- The training metrics of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../experiments/first_lstm_attempt/train_x.csv\", train_x, delimiter=\",\")\n",
    "np.savetxt(\"../experiments/first_lstm_attempt/train_y.csv\", train_y, delimiter=\",\")\n",
    "np.savetxt(\"../experiments/first_lstm_attempt/test_x.csv\", test_x, delimiter=\",\")\n",
    "np.savetxt(\"../experiments/first_lstm_attempt/test_y.csv\", test_y, delimiter=\",\")\n",
    "training_metrics_df.to_csv('../experiments/first_lstm_attempt/training_metrics.csv', index= False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving specific metrics of each model\n",
    "\n",
    "For each trained model we are going to save:\n",
    "\n",
    "- Predicted labels over the test_x\n",
    "- The output probability on the test_x prediction\n",
    "- The history of the training\n",
    "- The trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for training_result in training_results:\n",
    "    # Created the model folder\n",
    "    model_folder = \"../experiments/first_lstm_attempt/trained_models/\" + training_result[\"model_name\"]\n",
    "    if not os.path.exists(model_folder):\n",
    "        os.mkdir(model_folder)\n",
    "    \n",
    "    # Saves the predicted labels and the predicted probabilities on the test_x predictions\n",
    "    np.savetxt(model_folder + \"/predicted_y.csv\", training_result[\"predicted_y\"], delimiter=\",\")\n",
    "    np.savetxt(model_folder + \"/test_output_probabilities.csv\", training_result[\"test_output_probabilities\"], delimiter=\",\")\n",
    "    \n",
    "    # Creates a history dataframe and saves it in a file\n",
    "    history_df = pd.DataFrame(training_results[\"history\"].history)\n",
    "    history_df['epoch'] = training_results[\"history\"].epoch\n",
    "\n",
    "    history_df.to_csv(model_folder + \"/history.csv\", index= False, encoding='utf-8')\n",
    "    \n",
    "    # Saves the trained model in a file\n",
    "    training_result[\"model\"].save(filepath=model_folder + \"/model.hdf5\", overwrite=True, include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
